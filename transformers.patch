diff --git a/examples/pytorch/multiple-choice/run_swag.py b/examples/pytorch/multiple-choice/run_swag.py
index 64de16702..13b26af18 100755
--- a/examples/pytorch/multiple-choice/run_swag.py
+++ b/examples/pytorch/multiple-choice/run_swag.py
@@ -286,6 +286,7 @@ def main():
         cache_dir=model_args.cache_dir,
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
+        return_dict=False,
     )
     tokenizer = AutoTokenizer.from_pretrained(
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
diff --git a/examples/pytorch/question-answering/run_qa.py b/examples/pytorch/question-answering/run_qa.py
index cad46c5c2..9380e8fab 100755
--- a/examples/pytorch/question-answering/run_qa.py
+++ b/examples/pytorch/question-answering/run_qa.py
@@ -290,6 +290,7 @@ def main():
         cache_dir=model_args.cache_dir,
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
+        return_dict=False,
     )
     tokenizer = AutoTokenizer.from_pretrained(
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
diff --git a/examples/pytorch/question-answering/utils_qa.py b/examples/pytorch/question-answering/utils_qa.py
index 1157849c9..0bdd947a9 100644
--- a/examples/pytorch/question-answering/utils_qa.py
+++ b/examples/pytorch/question-answering/utils_qa.py
@@ -135,7 +135,9 @@ def postprocess_qa_predictions(
                         start_index >= len(offset_mapping)
                         or end_index >= len(offset_mapping)
                         or offset_mapping[start_index] is None
+                        or len(offset_mapping[start_index]) < 2
                         or offset_mapping[end_index] is None
+                        or len(offset_mapping[end_index]) < 2
                     ):
                         continue
                     # Don't consider answers with a length that is either < 0 or > max_answer_length.
diff --git a/examples/pytorch/text-classification/run_glue.py b/examples/pytorch/text-classification/run_glue.py
index 731da5023..52afa4449 100755
--- a/examples/pytorch/text-classification/run_glue.py
+++ b/examples/pytorch/text-classification/run_glue.py
@@ -319,6 +319,7 @@ def main():
         cache_dir=model_args.cache_dir,
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
+        return_dict=False,
     )
     tokenizer = AutoTokenizer.from_pretrained(
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
diff --git a/examples/pytorch/text-generation/run_generation.py b/examples/pytorch/text-generation/run_generation.py
index 9b4b09fc9..7a76b8e98 100755
--- a/examples/pytorch/text-generation/run_generation.py
+++ b/examples/pytorch/text-generation/run_generation.py
@@ -24,6 +24,9 @@ import logging
 import numpy as np
 import torch
 
+import time
+import os
+
 from transformers import (
     CTRLLMHeadModel,
     CTRLTokenizer,
@@ -196,6 +199,14 @@ def main():
         action="store_true",
         help="Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit",
     )
+    parser.add_argument("--num_warmup_iter", type=int, default=50, help="The number warmup, default is 50.")
+    parser.add_argument("--benchmark_iter", type=int, default=500, help="The number iters of benchmark, default is 500.")
+    parser.add_argument("--ipex", action="store_true", help="Use Intel IPEX to optimize.")
+    parser.add_argument("--jit", action="store_true", help="Use jit optimize to do optimization.")
+    parser.add_argument("--channels_last", type=bool, default=False, help="Use pytorch NHWC.")
+    parser.add_argument("--profile", type=bool, default=False, help="Trigger profile on current topology.")
+    parser.add_argument('--precision', default='float32', help='Precision, "float32" or "bfloat16"')
+
     args = parser.parse_args()
 
     args.device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
@@ -241,14 +252,41 @@ def main():
     else:
         prefix = args.prefix if args.prefix else args.padding_text
         encoded_prompt = tokenizer.encode(prefix + prompt_text, add_special_tokens=False, return_tensors="pt")
-    encoded_prompt = encoded_prompt.to(args.device)
 
     if encoded_prompt.size()[-1] == 0:
         input_ids = None
     else:
         input_ids = encoded_prompt
 
-    output_sequences = model.generate(
+    # encoded_prompt = encoded_prompt.to(args.device)
+    
+    if args.ipex:
+        # Import Extension
+        print("Now use Intel IPEX to optimize model.")
+        import intel_extension_for_pytorch as ipex
+        ipex.core.enable_auto_dnnl
+        model = ipex.optimize(model, dtype=torch.float32)
+        encoded_prompt = ipex.optimize(encoded_prompt, dtype=torch.float32)
+    if args.jit:
+        ipex.core.enable_jit_opt()
+        try:
+            #model = torch.jit.script(model)
+            model = torch.jit.trace(model, encoded_prompt)
+            print("---- With JIT enabled.")
+        except:
+            print("---- With JIT disabled.")
+    ### to oob
+    elif args.channels_last:
+        model_oob, input_oob = model, encoded_prompt
+        try:
+            model_oob = model_oob.to(memory_format=torch.channels_last)
+            input_oob = input_oob.to(memory_format=torch.channels_last)
+        except:
+            pass
+        model, encoded_prompt = model_oob, input_oob
+        
+    # warmup generate
+    _ = model.generate(
         input_ids=input_ids,
         max_length=args.length + len(encoded_prompt[0]),
         temperature=args.temperature,
@@ -256,8 +294,76 @@ def main():
         top_p=args.p,
         repetition_penalty=args.repetition_penalty,
         do_sample=True,
-        num_return_sequences=args.num_return_sequences,
-    )
+        num_return_sequences=args.num_warmup_iter,
+     )
+
+    # inference benchmark
+    tic  = time.time()
+    if args.precision == "bfloat16":
+        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+            output_sequences = model.generate(
+                input_ids=encoded_prompt,
+                max_length=args.length + len(encoded_prompt[0]),
+                temperature=args.temperature,
+                top_k=args.k,
+                top_p=args.p,
+                repetition_penalty=args.repetition_penalty,
+                do_sample=True,
+                num_return_sequences=args.benchmark_iter,
+            )
+    else:
+        output_sequences = model.generate(
+            input_ids=encoded_prompt,
+            max_length=args.length + len(encoded_prompt[0]),
+            temperature=args.temperature,
+            top_k=args.k,
+            top_p=args.p,
+            repetition_penalty=args.repetition_penalty,
+            do_sample=True,
+            num_return_sequences=args.benchmark_iter,
+        )
+    total_time = time.time() - tic
+    
+    print(" time cost %s\n inference Latency: %s s\n inference Throughput: %s samples/s\n "
+          %(total_time, total_time / args.num_return_sequences, args.num_return_sequences / total_time))
+    print("output latency: ", round(total_time / args.num_return_sequences, 3))
+    print("output throughput: ", round(args.num_return_sequences / total_time, 3))
+    print("output batch size: ", args.num_return_sequences)
+    
+    # profiler
+    if args.profile:
+        with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+            if args.precision == "bfloat16":
+                with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                    output_sequences = model.generate(
+                        input_ids=encoded_prompt,
+                        max_length=args.length + len(encoded_prompt[0]),
+                        temperature=args.temperature,
+                        top_k=args.k,
+                        top_p=args.p,
+                        repetition_penalty=args.repetition_penalty,
+                        do_sample=True,
+                        num_return_sequences=args.benchmark_iter,
+                    )
+            else:
+                output_sequences = model.generate(
+                    input_ids=encoded_prompt,
+                    max_length=args.length + len(encoded_prompt[0]),
+                    temperature=args.temperature,
+                    top_k=args.k,
+                    top_p=args.p,
+                    repetition_penalty=args.repetition_penalty,
+                    do_sample=True,
+                    num_return_sequences=args.benchmark_iter,
+                )
+
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + str(os.getpid()) + '.json'
+        prof.export_chrome_trace(timeline_file)
+        print(prof.key_averages().table(sort_by="self_cpu_time_total", row_limit=-1))
 
     # Remove the batch dimension when returning multiple sequences
     if len(output_sequences.shape) > 2:
@@ -266,7 +372,7 @@ def main():
     generated_sequences = []
 
     for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
-        print(f"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===")
+        #print(f"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===")
         generated_sequence = generated_sequence.tolist()
 
         # Decode text
@@ -281,7 +387,7 @@ def main():
         )
 
         generated_sequences.append(total_sequence)
-        print(total_sequence)
+        #print(total_sequence)
 
     return generated_sequences
 
diff --git a/examples/pytorch/token-classification/run_ner.py b/examples/pytorch/token-classification/run_ner.py
index f74ab6ec8..664b607ed 100755
--- a/examples/pytorch/token-classification/run_ner.py
+++ b/examples/pytorch/token-classification/run_ner.py
@@ -317,6 +317,7 @@ def main():
         cache_dir=model_args.cache_dir,
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
+        return_dict=False,
     )
 
     tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 38fba2823..f07796009 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -229,13 +229,13 @@ class ModuleUtilsMixin:
         # encoder_extended_attention_mask.transpose(-1, -2))
         encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility
 
-        if self.dtype == torch.float16:
+        if self.dtype == torch.float16 or self.dtype == torch.bfloat16:
             encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e4
         elif self.dtype == torch.float32:
             encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e9
         else:
             raise ValueError(
-                f"{self.dtype} not recognized. `dtype` should be set to either `torch.float32` or `torch.float16`"
+                f"{self.dtype} not recognized. `dtype` should be set to either `torch.float32` or `torch.float16` or `torch.bfloat16`"
             )
 
         return encoder_extended_attention_mask
diff --git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py
index f02d67a31..422d4625b 100755
--- a/src/transformers/models/bert/modeling_bert.py
+++ b/src/transformers/models/bert/modeling_bert.py
@@ -1502,13 +1502,13 @@ class BertForSequenceClassification(BertPreTrainedModel):
     )
     def forward(
         self,
-        input_ids=None,
+        labels=None,
         attention_mask=None,
+        input_ids=None,
         token_type_ids=None,
         position_ids=None,
         head_mask=None,
         inputs_embeds=None,
-        labels=None,
         output_attentions=None,
         output_hidden_states=None,
         return_dict=None,
diff --git a/src/transformers/models/distilbert/modeling_distilbert.py b/src/transformers/models/distilbert/modeling_distilbert.py
index 5d6deb138..edb2018ef 100755
--- a/src/transformers/models/distilbert/modeling_distilbert.py
+++ b/src/transformers/models/distilbert/modeling_distilbert.py
@@ -605,11 +605,11 @@ class DistilBertForSequenceClassification(DistilBertPreTrainedModel):
     )
     def forward(
         self,
-        input_ids=None,
+        labels=None,
         attention_mask=None,
+        input_ids=None,
         head_mask=None,
         inputs_embeds=None,
-        labels=None,
         output_attentions=None,
         output_hidden_states=None,
         return_dict=None,
diff --git a/src/transformers/models/roberta/modeling_roberta.py b/src/transformers/models/roberta/modeling_roberta.py
index 09472da76..04e4dbe3f 100644
--- a/src/transformers/models/roberta/modeling_roberta.py
+++ b/src/transformers/models/roberta/modeling_roberta.py
@@ -1171,13 +1171,13 @@ class RobertaForSequenceClassification(RobertaPreTrainedModel):
     )
     def forward(
         self,
-        input_ids=None,
+        labels=None,
         attention_mask=None,
+        input_ids=None,
         token_type_ids=None,
         position_ids=None,
         head_mask=None,
         inputs_embeds=None,
-        labels=None,
         output_attentions=None,
         output_hidden_states=None,
         return_dict=None,
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index f59cec776..f8b25d5bb 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -175,6 +175,8 @@ if is_sagemaker_mp_enabled():
 if TYPE_CHECKING:
     import optuna
 
+import time
+
 logger = logging.get_logger(__name__)
 
 
@@ -1256,7 +1258,6 @@ class Trainer:
             self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)
 
             for step, inputs in enumerate(epoch_iterator):
-
                 # Skip past any already trained steps if resuming training
                 if steps_trained_in_current_epoch > 0:
                     steps_trained_in_current_epoch -= 1
@@ -1784,9 +1785,17 @@ class Trainer:
 
         if self.use_amp:
             with autocast():
-                loss = self.compute_loss(model, inputs)
+                if self.args.precision == 'bfloat16':
+                    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                        loss = self.compute_loss(model, inputs)
+                else:
+                    loss = self.compute_loss(model, inputs)
         else:
-            loss = self.compute_loss(model, inputs)
+            if self.args.precision == 'bfloat16':
+                with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                    loss = self.compute_loss(model, inputs)
+            else:
+                loss = self.compute_loss(model, inputs)
 
         if self.args.n_gpu > 1:
             loss = loss.mean()  # mean() to average on multi-gpu parallel training
@@ -1818,7 +1827,18 @@ class Trainer:
             labels = inputs.pop("labels")
         else:
             labels = None
-        outputs = model(**inputs)
+        
+            if self.args.torchdynamo_ipex:
+                import torchdynamo
+                from torchdynamo.optimizations import backends
+                if self.args.precision == "float32":
+                    with torchdynamo.optimize(backends.ipex_fp32), torch.no_grad():
+                        outputs = model(**inputs)
+                if self.args.precision == "bfloat16":
+                    with torchdynamo.optimize(backends.ipex_bf16), torch.no_grad(), torch.cpu.amp.autocast():
+                        outputs = model(**inputs)
+            else:
+                outputs = model(**inputs)
         # Save past state if it exists
         # TODO: this needs to be fixed and made cleaner later.
         if self.args.past_index >= 0:
@@ -2048,15 +2068,28 @@ class Trainer:
         start_time = time.time()
 
         eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
-        output = eval_loop(
-            eval_dataloader,
-            description="Evaluation",
-            # No point gathering the predictions if there are no metrics, otherwise we defer to
-            # self.args.prediction_loss_only
-            prediction_loss_only=True if self.compute_metrics is None else None,
-            ignore_keys=ignore_keys,
-            metric_key_prefix=metric_key_prefix,
-        )
+        print("[INFO|benchmark log] Running evaluate() ...")
+        if self.args.use_shared_weight:
+            print("[INFO|benchmark log] Running weight sharing mode ...")
+            import threading
+            threads = []
+            num_instances = self.args.total_cores // self.args.cores_per_instance
+            for i in range(0, num_instances):
+                t = threading.Thread(target=eval_loop, args=(eval_dataloader, "Evaluation", True, ignore_keys, metric_key_prefix))
+                threads.append(t)
+                t.start()
+            for t in threads:
+                t.join()
+        else:
+            output = eval_loop(
+                eval_dataloader,
+                description="Evaluation",
+                # No point gathering the predictions if there are no metrics, otherwise we defer to
+                # self.args.prediction_loss_only
+                prediction_loss_only=True if self.compute_metrics is None else None,
+                ignore_keys=ignore_keys,
+                metric_key_prefix=metric_key_prefix,
+            )
 
         total_batch_size = self.args.eval_batch_size * self.args.world_size
         output.metrics.update(
@@ -2120,6 +2153,7 @@ class Trainer:
         start_time = time.time()
 
         eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
+        print("[INFO|benchmark log] Running predict() ...")
         output = eval_loop(
             test_dataloader, description="Prediction", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix
         )
@@ -2210,62 +2244,461 @@ class Trainer:
 
         observed_num_examples = 0
         # Main evaluation loop
-        for step, inputs in enumerate(dataloader):
-            # Update the observed num examples
-            observed_batch_size = find_batch_size(inputs)
-            if observed_batch_size is not None:
-                observed_num_examples += observed_batch_size
-                # For batch samplers, batch_size is not known by the dataloader in advance.
-                if batch_size is None:
-                    batch_size = observed_batch_size
-
-            # Prediction step
-            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
-
-            # Update containers on host
-            if loss is not None:
-                losses = self._nested_gather(loss.repeat(batch_size))
-                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
-            if logits is not None:
-                logits = self._pad_across_processes(logits)
-                logits = self._nested_gather(logits)
-                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
-            if labels is not None:
-                labels = self._pad_across_processes(labels)
-                labels = self._nested_gather(labels)
-                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
-            self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
-
-            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
-            if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
-                if losses_host is not None:
-                    losses = nested_numpify(losses_host)
-                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)
-                if preds_host is not None:
-                    logits = nested_numpify(preds_host)
-                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)
-                if labels_host is not None:
-                    labels = nested_numpify(labels_host)
-                    all_labels = (
-                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)
-                    )
-
-                # Set back to None to begin a new accumulation
-                losses_host, preds_host, labels_host = None, None, None
+        total_time = 0
+        total_data = 0
+        
+        if self.args.ipex:
+            # Import Extension
+            import intel_extension_for_pytorch as ipex
+            print("[INFO|benchmark log] Running with IPEX...")
+            if self.args.precision == 'bfloat16':
+                # Automatically mix precision
+                model = ipex.optimize(model, dtype=torch.bfloat16) #ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+                print("[INFO|benchmark log] Running with bfloat16...")
+            model = ipex.optimize(model, dtype=torch.float32)
+        if self.args.channels_last:
+            model_oob = model
+            try:
+                model_oob = model_oob.to(memory_format=torch.channels_last)
+            except:
+                pass
+            model = model_oob
+        if self.args.jit:
+            jit_inputs=()
+            for _,batch in enumerate(dataloader):
+                for _,label in enumerate(batch): 
+                    if (batch[label].dim()) >=4:
+                        dumpy_tensor = torch.ones((batch[label].shape), dtype=torch.long).to(memory_format=torch.channels_last)
+                    else:
+                        dumpy_tensor = torch.ones((batch[label].shape), dtype=torch.long)
+                    L1=list(jit_inputs)
+                    L1.append(dumpy_tensor)
+                    jit_inputs=tuple(L1)
+                break
+            if self.args.jit_optimize: 
+                model = torch.jit.optimize_for_inference(torch.jit.trace(model, jit_inputs, strict=False))
+                print("[INFO|benchmark log] Running with JIT and jit.optimize_for_inference API ...")
+            else:
+                model = torch.jit.trace(model, jit_inputs, strict=False)
+                model = torch.jit.freeze(model)
+                print("[INFO|benchmark log] Running with JIT ...")
+        if self.args.precision == 'int8':
+            if self.args.int8_calibration:
+                ### calibration
+                print("[INFO|benchmark log] Started int8 calibration ...")
+                import intel_extension_for_pytorch as ipex
+                conf = ipex.quantization.QuantConf(qscheme=torch.per_tensor_affine)
+                for step, inputs in enumerate(dataloader):
+                    print("calibration step: {}".format(step))
+                    # Update the observed num examples
+                    observed_batch_size = find_batch_size(inputs)
+                    if observed_batch_size is not None:
+                        observed_num_examples += observed_batch_size
+                        # For batch samplers, batch_size is not known by the dataloader in advance.
+                        if batch_size is None:
+                           batch_size = observed_batch_size
+                    with ipex.quantization.calibrate(conf):
+                        self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                    if step == 1:
+                        conf.save("configure.json")
+                        break
+                print("[INFO|benchmark log] Finished int8 calibration ...")
+            ### quantization
+            print("[INFO|benchmark log] Started int8 quantization ...")
+            jit_inputs=()
+            example_input = None
+            for _,batch in enumerate(dataloader):
+                example_input = batch
+                for _,label in enumerate(batch): 
+                    if (batch[label].dim()) >=4:
+                        dumpy_tensor = torch.ones((batch[label].shape), dtype=torch.long).to(memory_format=torch.channels_last)
+                    else:
+                        dumpy_tensor = torch.ones((batch[label].shape), dtype=torch.long)
+                    #if  dumpy_tensor.shape!=torch.Size([1]):
+                    L1=list(jit_inputs)
+                    L1.append(dumpy_tensor)
+                    jit_inputs=tuple(L1)
+                break
+            conf = ipex.quantization.QuantConf(configure_file="configure.json")
+            # convert model to trace model.
+            model = ipex.quantization.convert(model, conf, jit_inputs)
+            # enable fusion path work(need to run two interation).
+            with torch.no_grad():
+                y = model(**batch)
+                y = model(**batch)
+            print("[INFO|benchmark log] Finished int8 quantization ...")
+            print("[INFO|benchmark log] Running int8 ...")
+        batch_size_ = 0
+        if self.args.precision == 'bfloat16':
+            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                if not self.args.dnnlverbose:
+                    itcount = 0
+                    num_loop = 1
+                    if ((self.num_examples(dataloader)//batch_size)+1) < self.args.minimum_iter:
+                        num_loop = self.args.minimum_iter//((self.num_examples(dataloader)//batch_size)+1)
+                    total_iter = num_loop*((self.num_examples(dataloader)//batch_size)+1)
+                    t_arr = np.empty((0,1))
+                    # print("total_iter = ", total_iter)
+                    for step, inputs in enumerate(dataloader):
+                        if itcount == self.args.early_stop_at_iter or step == (self.num_examples(dataloader)//batch_size):
+                            break
+                        for repeat in range(num_loop):
+                            itcount = itcount + 1
+                            # Update the observed num examples
+                            observed_batch_size = find_batch_size(inputs)
+                            if observed_batch_size is not None:
+                                observed_num_examples += observed_batch_size
+                                # For batch samplers, batch_size is not known by the dataloader in advance.
+                                if batch_size is None:
+                                    batch_size = observed_batch_size
+                            if self.args.ipex:
+                                inputs = inputs #inputs = {key:ipex.optimize(value) for key,value in inputs.items()}
+                            if self.args.channels_last:
+                                input_oob = inputs
+                                try:
+                                    input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in input_oob.items()}
+                                except:
+                                    pass
+                                inputs = input_oob
+
+                            # Prediction step
+                            tic = time.time()
+                            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                            toc = time.time()
+                            #loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+
+                            # Update containers on host
+                            if loss is not None:
+                                losses = self._nested_gather(loss.repeat(batch_size))
+                                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
+                            if logits is not None:
+                                logits = self._pad_across_processes(logits)
+                                logits = self._nested_gather(logits)
+                                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
+                            if labels is not None:
+                                labels = self._pad_across_processes(labels)
+                                labels = self._nested_gather(labels)
+                                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
+                            self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
+                                
+                            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
+                            if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
+                                if losses_host is not None:
+                                    losses = nested_numpify(losses_host, self.args.precision)
+                                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)
+                                if preds_host is not None:
+                                    logits = nested_numpify(preds_host, self.args.precision)
+                                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)
+                                if labels_host is not None:
+                                    labels = nested_numpify(labels_host, self.args.precision)
+                                    all_labels = (
+                                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)
+                                    )
+
+                                # Set back to None to begin a new accumulation
+                                losses_host, preds_host, labels_host = None, None, None
+                            # toc = time.time()
+                            # print("elapsed time = ", toc - tic)
+                            if itcount > self.args.num_warmup_iter and itcount < (total_iter - self.args.num_warmup_iter):
+                                # print("------------------------------elapsed time for calc = ", toc - tic)
+                                t_arr = np.append(t_arr, toc - tic)
+                                total_time += toc - tic
+                                total_data += batch_size
+                                batch_size_ = batch_size
+                # print("t_arr = ", t_arr)
+                print("t_arr var. = ", np.std(t_arr)/np.mean(t_arr))
+                print(" time cost %s\n total samples %s \n inference latency: %s s\n inference Throughput: %s images/s\n " %(total_time, total_data, total_time / total_data, total_data / total_time))
+                print("output latency: ", round(total_time /total_data, 3))
+                print("output throughput: ", round(total_data / total_time, 3))
+                print("output batch size: ", batch_size_)
+        else:
+            if not self.args.dnnlverbose:
+                itcount = 0
+                num_loop = 1
+                if ((self.num_examples(dataloader)//batch_size)+1) < self.args.minimum_iter:
+                    num_loop = self.args.minimum_iter//((self.num_examples(dataloader)//batch_size)+1)
+                total_iter = num_loop*((self.num_examples(dataloader)//batch_size)+1)
+                t_arr = np.empty((0,1))
+                # print("total_iter = ", total_iter)
+                for step, inputs in enumerate(dataloader):
+                    if itcount == self.args.early_stop_at_iter or step == (self.num_examples(dataloader)//batch_size):
+                        break
+                    for repeat in range(num_loop):
+                        itcount = itcount + 1
+                        # Update the observed num examples
+                        observed_batch_size = find_batch_size(inputs)
+                        if observed_batch_size is not None:
+                            observed_num_examples += observed_batch_size
+                            # For batch samplers, batch_size is not known by the dataloader in advance.
+                            if batch_size is None:
+                                batch_size = observed_batch_size
+                        if self.args.ipex:
+                            inputs = inputs #inputs = {key:ipex.optimize(value) for key,value in inputs.items()}
+                        if self.args.channels_last:
+                            input_oob = inputs
+                            try:
+                                input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in input_oob.items()}
+                            except:
+                                pass
+                            inputs = input_oob
+                        
+                        # Prediction step
+                        tic = time.time()
+                        loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                        toc = time.time()
+                        #loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+
+                        # Update containers on host
+                        if loss is not None:
+                            losses = self._nested_gather(loss.repeat(batch_size))
+                            losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
+                        if logits is not None:
+                            logits = self._pad_across_processes(logits)
+                            logits = self._nested_gather(logits)
+                            preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
+                        if labels is not None:
+                            labels = self._pad_across_processes(labels)
+                            labels = self._nested_gather(labels)
+                            labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
+                        self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
+                            
+                        # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
+                        if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
+                            if losses_host is not None:
+                                losses = nested_numpify(losses_host, self.args.precision)
+                                all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)
+                            if preds_host is not None:
+                                logits = nested_numpify(preds_host, self.args.precision)
+                                all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)
+                            if labels_host is not None:
+                                labels = nested_numpify(labels_host, self.args.precision)
+                                all_labels = (
+                                    labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)
+                                )
+
+                            # Set back to None to begin a new accumulation
+                            losses_host, preds_host, labels_host = None, None, None
+                        # toc = time.time()
+                        # print("elapsed time = ", toc - tic)
+                        if itcount > self.args.num_warmup_iter and itcount < (total_iter - self.args.num_warmup_iter):
+                            # print("------------------------------elapsed time for calc = ", toc - tic)
+                            t_arr = np.append(t_arr, toc - tic)
+                            total_time += toc - tic
+                            total_data += batch_size
+                            batch_size_ = batch_size
+            # print("t_arr = ", t_arr)
+            print("t_arr var. = ", np.std(t_arr)/np.mean(t_arr))
+            print(" time cost %s\n total samples %s \n inference latency: %s s\n inference Throughput: %s images/s\n " %(total_time, total_data, total_time / total_data, total_data / total_time))
+            print("output latency: ", round(total_time /total_data, 3))
+            print("output throughput: ", round(total_data / total_time, 3))
+            print("output batch size: ", batch_size_)
+        
+        if self.args.precision == 'bfloat16':
+            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                if self.args.dnnlverbose:
+                    dnnlnumiter = self.args.dnnlverbose_numiter
+                    countdnnliter = 0
+                    itcount = 0
+                    for step, inputs in enumerate(dataloader):
+                        itcount = itcount + 1
+                        if itcount == self.args.early_stop_at_iter:
+                            break
+                        countdnnliter = countdnnliter + 1
+                        if countdnnliter > dnnlnumiter :
+                            break
+                        # Update the observed num examples
+                        observed_batch_size = find_batch_size(inputs)
+                        if observed_batch_size is not None:
+                            observed_num_examples += observed_batch_size
+                            # For batch samplers, batch_size is not known by the dataloader in advance.
+                            if batch_size is None:
+                                batch_size = observed_batch_size
+                        if self.args.ipex:
+                            inputs = inputs #inputs = {key:ipex.optimize(value) for key,value in inputs.items()}
+                        if self.args.channels_last:
+                            input_oob = inputs
+                            try:
+                                input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in input_oob.items()}
+                            except:
+                                pass
+                            inputs = input_oob
+                            
+                        # Prediction step
+                        loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                        print("ran ", countdnnliter, " iterations for dnnlverbose")
+
+                        # Update containers on host
+                        if loss is not None:
+                            losses = self._nested_gather(loss.repeat(batch_size))
+                            losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
+                        if logits is not None:
+                            logits = self._pad_across_processes(logits)
+                            logits = self._nested_gather(logits)
+                            preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
+                        if labels is not None:
+                            labels = self._pad_across_processes(labels)
+                            labels = self._nested_gather(labels)
+                            labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
+                        self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
+                        # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
+                        if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
+                            if losses_host is not None:
+                                losses = nested_numpify(losses_host, self.args.precision)
+                                all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)
+                            if preds_host is not None:
+                                logits = nested_numpify(preds_host, self.args.precision)
+                                all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)
+                            if labels_host is not None:
+                                labels = nested_numpify(labels_host, self.args.precision)
+                                all_labels = (
+                                    labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)
+                                )
+
+                            # Set back to None to begin a new accumulation
+                            losses_host, preds_host, labels_host = None, None, None
+        else:
+            if self.args.dnnlverbose:
+                dnnlnumiter = self.args.dnnlverbose_numiter
+                countdnnliter = 0
+                itcount = 0
+                for step, inputs in enumerate(dataloader):
+                    itcount = itcount + 1
+                    if itcount == self.args.early_stop_at_iter:
+                        break
+                    countdnnliter = countdnnliter + 1
+                    if countdnnliter > dnnlnumiter :
+                        break
+                    # Update the observed num examples
+                    observed_batch_size = find_batch_size(inputs)
+                    if observed_batch_size is not None:
+                        observed_num_examples += observed_batch_size
+                        # For batch samplers, batch_size is not known by the dataloader in advance.
+                        if batch_size is None:
+                            batch_size = observed_batch_size
+                    if self.args.ipex:
+                        inputs = inputs #inputs = {key:ipex.optimize(value) for key,value in inputs.items()}
+                    if self.args.channels_last:
+                        input_oob = inputs
+                        try:
+                            input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in input_oob.items()}
+                        except:
+                            pass
+                        inputs = input_oob
+                        
+                    # Prediction step
+                    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                    print("ran ", countdnnliter, " iterations for dnnlverbose")
+
+                    # Update containers on host
+                    if loss is not None:
+                        losses = self._nested_gather(loss.repeat(batch_size))
+                        losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
+                    if logits is not None:
+                        logits = self._pad_across_processes(logits)
+                        logits = self._nested_gather(logits)
+                        preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
+                    if labels is not None:
+                        labels = self._pad_across_processes(labels)
+                        labels = self._nested_gather(labels)
+                        labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
+                    self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
+                    # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
+                    if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
+                        if losses_host is not None:
+                            losses = nested_numpify(losses_host, self.args.precision)
+                            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)
+                        if preds_host is not None:
+                            logits = nested_numpify(preds_host, self.args.precision)
+                            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)
+                        if labels_host is not None:
+                            labels = nested_numpify(labels_host, self.args.precision)
+                            all_labels = (
+                                labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)
+                            )
 
+                        # Set back to None to begin a new accumulation
+                        losses_host, preds_host, labels_host = None, None, None
+        
+        if self.args.precision == 'bfloat16':
+            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):                
+                if self.args.profile:
+                    #do profiler for only 1 iteration:
+                    for step, inputs in enumerate(dataloader):
+                        # Update the observed num examples
+                        observed_batch_size = find_batch_size(inputs)
+                        if observed_batch_size is not None:
+                            observed_num_examples += observed_batch_size
+                            # For batch samplers, batch_size is not known by the dataloader in advance.
+                            if batch_size is None:
+                                batch_size = observed_batch_size
+                        if self.args.ipex:
+                            inputs = inputs #inputs = {key:ipex.optimize(value) for key,value in inputs.items()}
+                        if self.args.channels_last:
+                            input_oob = inputs
+                            try:
+                                input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in input_oob.items()}
+                            except:
+                                pass
+                            inputs = input_oob
+                        
+                        # Prediction step with Profiler
+                        with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                        
+                        break
+                    import pathlib
+                    timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+                    if not os.path.exists(timeline_dir):
+                        os.makedirs(timeline_dir)
+                    timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + str(os.getpid()) + '.json'
+                    prof.export_chrome_trace(timeline_file)
+                    print(prof.key_averages().table(sort_by="self_cpu_time_total", row_limit=-1))
+        else:
+            if self.args.profile:
+                #do profiler for only 1 iteration:
+                for step, inputs in enumerate(dataloader):
+                    # Update the observed num examples
+                    observed_batch_size = find_batch_size(inputs)
+                    if observed_batch_size is not None:
+                        observed_num_examples += observed_batch_size
+                        # For batch samplers, batch_size is not known by the dataloader in advance.
+                        if batch_size is None:
+                            batch_size = observed_batch_size
+                    if self.args.ipex:
+                        inputs = inputs #inputs = {key:ipex.optimize(value) for key,value in inputs.items()}
+                    if self.args.channels_last:
+                        input_oob = inputs
+                        try:
+                            input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in input_oob.items()}
+                        except:
+                            pass
+                        inputs = input_oob
+                    
+                    # Prediction step with Profiler
+                    with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                        loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                    
+                    break
+                import pathlib
+                timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+                if not os.path.exists(timeline_dir):
+                    os.makedirs(timeline_dir)
+                timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + str(os.getpid()) + '.json'
+                prof.export_chrome_trace(timeline_file)
+                print(prof.key_averages().table(sort_by="self_cpu_time_total", row_limit=-1))
         if self.args.past_index and hasattr(self, "_past"):
             # Clean the state at the end of the evaluation loop
             delattr(self, "_past")
 
         # Gather all remaining tensors and put them back on the CPU
         if losses_host is not None:
-            losses = nested_numpify(losses_host)
+            losses = nested_numpify(losses_host, self.args.precision)
             all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)
         if preds_host is not None:
-            logits = nested_numpify(preds_host)
+            logits = nested_numpify(preds_host, self.args.precision)
             all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)
         if labels_host is not None:
-            labels = nested_numpify(labels_host)
+            labels = nested_numpify(labels_host, self.args.precision)
             all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)
 
         # Number of samples
@@ -2289,7 +2722,7 @@ class Trainer:
 
         # Metrics!
         if self.compute_metrics is not None and all_preds is not None and all_labels is not None:
-            metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
+            metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels.astype(np.int)))
         else:
             metrics = {}
 
@@ -2432,9 +2865,19 @@ class Trainer:
                         logits = outputs[1:]
                 else:
                     loss = None
-                    if self.use_amp:
-                        with autocast():
-                            outputs = model(**inputs)
+                    # if self.use_amp:
+                        # with autocast():
+                            # outputs = model(**inputs)
+                    # else:
+                    if self.args.torchdynamo_ipex:
+                        import torchdynamo
+                        from torchdynamo.optimizations import backends
+                        if self.args.precision == "float32":
+                            with torchdynamo.optimize(backends.ipex_fp32), torch.no_grad():
+                                outputs = model(**inputs)
+                        if self.args.precision == "bfloat16":
+                            with torchdynamo.optimize(backends.ipex_bf16), torch.no_grad(), torch.cpu.amp.autocast():
+                                outputs = model(**inputs)
                     else:
                         outputs = model(**inputs)
                     if isinstance(outputs, dict):
@@ -2622,27 +3065,65 @@ class Trainer:
             self._past = None
 
         self.callback_handler.eval_dataloader = dataloader
-
-        for step, inputs in enumerate(dataloader):
-            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
-            if loss is not None:
-                losses = loss.repeat(batch_size)
-                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
-            if logits is not None:
-                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
-            if labels is not None:
-                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
-            self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
-
-            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
-            if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
-                eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, "eval_losses"))
-                if not prediction_loss_only:
-                    preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, "eval_preds"))
-                    labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, "eval_label_ids"))
-
-                # Set back to None to begin a new accumulation
-                losses_host, preds_host, labels_host = None, None, None
+        if self.args.precision == 'bfloat16':
+            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                itcount = 0
+                num_loop = 1
+                if ((self.num_examples(dataloader)//batch_size)+1) < self.args.minimum_iter:
+                    num_loop = self.args.minimum_iter//((self.num_examples(dataloader)//batch_size)+1)
+                for repeat in range(num_loop):
+                    for step, inputs in enumerate(dataloader):
+                        itcount = itcount + 1
+                        if itcount == self.args.early_stop_at_iter:
+                            break
+                        loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                        if loss is not None:
+                            losses = loss.repeat(batch_size)
+                            losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
+                        if logits is not None:
+                            preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
+                        if labels is not None:
+                            labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
+                        self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
+
+                        # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
+                        if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
+                            eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, "eval_losses"))
+                            if not prediction_loss_only:
+                                preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, "eval_preds"))
+                                labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, "eval_label_ids"))
+
+                            # Set back to None to begin a new accumulation
+                            losses_host, preds_host, labels_host = None, None, None
+        else:
+            itcount = 0
+            num_loop = 1
+            if ((self.num_examples(dataloader)//batch_size)+1) < self.args.minimum_iter:
+                num_loop = self.args.minimum_iter//((self.num_examples(dataloader)//batch_size)+1)
+            for repeat in range(num_loop):
+                for step, inputs in enumerate(dataloader):
+                    itcount = itcount + 1
+                    if itcount == self.args.early_stop_at_iter or step == (self.num_examples(dataloader)//batch_size):
+                        break
+                    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                    if loss is not None:
+                        losses = loss.repeat(batch_size)
+                        losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)
+                    if logits is not None:
+                        preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
+                    if labels is not None:
+                        labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
+                    self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)
+
+                    # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
+                    if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:
+                        eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, "eval_losses"))
+                        if not prediction_loss_only:
+                            preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, "eval_preds"))
+                            labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, "eval_label_ids"))
+
+                        # Set back to None to begin a new accumulation
+                        losses_host, preds_host, labels_host = None, None, None
 
         if self.args.past_index and hasattr(self, "_past"):
             # Clean the state at the end of the evaluation loop
@@ -2690,4 +3171,4 @@ class Trainer:
         elif self.args.local_rank != -1:
             tensors = distributed_concat(tensors)
 
-        return nested_numpify(tensors)
+        return nested_numpify(tensors, self.args.precision)
diff --git a/src/transformers/trainer_pt_utils.py b/src/transformers/trainer_pt_utils.py
index ba0a49297..8f2e52ea6 100644
--- a/src/transformers/trainer_pt_utils.py
+++ b/src/transformers/trainer_pt_utils.py
@@ -132,12 +132,16 @@ def find_batch_size(tensors):
         return tensors.shape[0] if len(tensors.shape) >= 1 else None
 
 
-def nested_numpify(tensors):
+def nested_numpify(tensors, precision):
     "Numpify `tensors` (even if it's a nested list/tuple of tensors)."
     if isinstance(tensors, (list, tuple)):
-        return type(tensors)(nested_numpify(t) for t in tensors)
-    return tensors.cpu().numpy()
-
+        return type(tensors)(nested_numpify(t, precision) for t in tensors)
+    if precision == 'bfloat16':
+        return tensors.cpu().float().numpy()
+    elif precision == 'int8':
+        return tensors.cpu().float().numpy()
+    else:
+        return tensors.cpu().numpy()
 
 def nested_detach(tensors):
     "Detach `tensors` (even if it's a nested list/tuple of tensors)."
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 5f6b877bb..ba7deb0ad 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -625,6 +625,24 @@ class TrainingArguments:
         metadata={"help": "Used by the SageMaker launcher to send mp-specific args. Ignored in Trainer"},
     )
 
+    ipex: bool = field(default=False, metadata={"help": "Use Intel IPEX."})
+    torchdynamo_ipex: bool = field(default=False, metadata={"help": "Use Intel IPEX via torchdynamo."})
+    jit: bool = field(default=False, metadata={"help": "Use jit.trace to do optimization."})
+    use_shared_weight: bool = field(default=False, metadata={"help": "Use weight sharing mode."})
+    jit_optimize: bool = field(default=False, metadata={"help": "Use jit.optimize_for_inference on jit.trace to do optimization."})
+    jit_model_name: str = field(default='bert', metadata={"help": "model for jit trace"})
+    channels_last: bool = field(default=False, metadata={"help": "Use Pytorch NHWC."})
+    profile: bool = field(default=False, metadata={"help": "Trigger profile on current topology."})
+    dnnlverbose: bool = field(default=False, metadata={"help": "Trigger dnnlverbose on current topology."})
+    dnnlverbose_numiter: int = field(default=1, metadata={"help": "Number of iteration to trigger dnnlverbose on current topology."})
+    early_stop_at_iter: int = field(default=-1, metadata={"help": "At which iteration to stop the loop if the model or dataset is too large."})
+    minimum_iter: int = field(default=1, metadata={"help": "Minimum number of iterations. If num_example/batch_size is smaller than this num, it will add loops."})
+    num_warmup_iter: int = field(default=0, metadata={"help": "Warmup steps for evaluation benchmarking."})
+    precision: str = field(default='float32', metadata={"help": "float32, bfloat16, int8."})
+    int8_calibration: bool = field(default=False, metadata={"help": "Do calibration for IPEX INT8."})
+    total_cores: int = field(default=56, metadata={"help": "Number of total cores for weight sharing mode."})
+    cores_per_instance: int = field(default=4, metadata={"help": "Number of cores per instance for weight sharing mode."})
+
     def __post_init__(self):
         # Handle --use_env option in torch.distributed.launch (local_rank not passed as an arg then).
         # This needs to happen before any call to self.device or self.n_gpu.
